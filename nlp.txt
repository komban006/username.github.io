Lab 1: Tokenization, Lemmatization, Stemming and Stop Words Removal
Question:

To develop a python code to perform the Tokenization, stemming and lemmatization of given paragraph.


import nltk
from nltk.tokenize import TreebankWordTokenizer
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
import re

nltk.download('wordnet')
nltk.download('stopwords')

text = "Any sentence you like"
tokenizer = TreebankWordTokenizer()
tokens = tokenizer.tokenize(text)
print("Tokenized Words:", tokens)

stemmer = PorterStemmer()
stems = [stemmer.stem(word) for word in tokens]
print("Stemmed Words:", stems)

lemmatizer = WordNetLemmatizer()
lemmas = [lemmatizer.lemmatize(word) for word in tokens]
print("Lemmatized Words:", lemmas)

stop_words = set(stopwords.words('english'))
filtered = [word for word in tokens if word.lower() not in stop_words]
print("After Stop Word Removal:", filtered)

print("Words only:", re.findall(r'\b\w+\b', text))
print("Split by punctuation:", re.split(r'[.!?]', text))
print("Substitute 'text' with 'data':", re.sub(r'text', 'data', text))



--------------------------------------------------------------------------------


Lab 2: Bag of Words, TF-IDF
Question:

Implement Bag of words and TF-IDF algorithm on a sample text.


from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

corpus = [
    'Natural Language Processing is fascinating.',
    'Language processing with machine learning is powerful.'
]

# Bag of Words
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
print("Bag of Words:\n", X.toarray())
print("Vocabulary:\n", vectorizer.vocabulary_)

# TF-IDF
tfidf = TfidfVectorizer()
X_tfidf = tfidf.fit_transform(corpus)
print("TF-IDF:\n", X_tfidf.toarray())
print("TF-IDF Vocabulary:\n", tfidf.vocabulary_)




----------------------------------------------------------------------------------




Lab 3: Word2Vec, Word Embeddings
Question:

To develop a Python code to perform the below tasks:
Word2Vec and Word Embedding

import nltk
from nltk.corpus import brown
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# Download Brown corpus
nltk.download('brown')
sentences = [' '.join(sent) for sent in brown.sents()]

# Tokenize
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)
padded = pad_sequences(sequences, padding='post')

# Build model and extract weights
vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 100
model = Sequential([Embedding(input_dim=vocab_size, output_dim=embedding_dim)])
model.build(input_shape=(None, padded.shape[1]))
weights = model.layers[0].get_weights()[0]

# Words to display
words_to_show = ['king', 'queen', 'man', 'woman', 'apple', 'banana']
print("Word Embedding Vectors:\n")
for word in words_to_show:
    idx = tokenizer.word_index.get(word)
    if idx and idx < len(weights):
        vec = weights[idx]
        print(f"{word:7s}: [{', '.join(f'{v:.4f}' for v in vec[:10])} ...]")
    else:
        print(f"{word:7s}: not in vocabulary")

# Visualize top words
top50 = list(tokenizer.word_index.keys())[:50]
vecs = np.array([weights[tokenizer.word_index[w]] for w in top50])
coords = PCA(n_components=2).fit_transform(vecs)

plt.figure(figsize=(10,6))
plt.scatter(coords[:,0], coords[:,1])
for i, w in enumerate(top50):
    plt.annotate(w, (coords[i,0], coords[i,1]))
plt.title("Word Embeddings (Untrained)")
plt.show()










----------------------------------------------------------------------------------



Lab 4: Minimum Edit Distance Table
Question:

Find the minimum edit distance between two strings with Levenshtein distance


def edit_distance(str1, str2):
    m = len(str1)
    n = len(str2)
    dp = [[0 for x in range(n + 1)] for x in range(m + 1)]
    
    for i in range(m + 1):
        for j in range(n + 1):
            if i == 0:
                dp[i][j] = j
            elif j == 0:
                dp[i][j] = i
            elif str1[i - 1] == str2[j - 1]:
                dp[i][j] = dp[i - 1][j - 1]
            else:
                dp[i][j] = 1 + min(
                    dp[i - 1][j],    # Delete
                    dp[i][j - 1],    # Insert
                    dp[i - 1][j - 1] # Replace
                )

    print("Edit Distance Matrix:")
    print("   ", end="")
    for ch in " " + str2:
        print(f"{ch:>3}", end="")
    print()
    for i in range(m + 1):
        print(f"{str1[i - 1] if i > 0 else ' ':>3}", end="")
        for j in range(n + 1):
            print(f"{dp[i][j]:>3}", end="")
        print()
    
    return dp[m][n]

str1 = "intention"
str2 = "execution"
distance = edit_distance(str1, str2)
print("\nEdit Distance:", distance)






----------------------------------------------------------------------------------



Lab 5: N-Gram Models
Question:

Write a Python code to analyze Bigram, Trigram and N gram model




from nltk import ngrams
from collections import Counter

text = "Natural language processing is a part of AI"
tokens = text.lower().split()

def generate_ngrams(tokens, n):
    n_grams = list(ngrams(tokens, n))
    return n_grams

print("Bigrams:")
bigrams = generate_ngrams(tokens, 2)
print(bigrams)
print("\nFrequency of Bigrams:")
print(Counter(bigrams))

print("\nTrigrams:")
trigrams = generate_ngrams(tokens, 3)
print(trigrams)
print("\nFrequency of Trigrams:")
print(Counter(trigrams))

n = int(input("\nEnter the value of n for N-gram model: "))
print(f"\n{n}-grams:")
n_grams = generate_ngrams(tokens, n)
print(n_grams)
print(f"\nFrequency of {n}-grams:")
print(Counter(n_grams))





--------------------------------------------------------------------------------



Lab 6: Cosine for Measuring Similarity
Question:

To develop a python code to perform Cosine Similarity


from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

text1 = "Natural Language Processing"
text2 = "Natural Language Understanding"

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform([text1, text2])

cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)
print("Cosine Similarity:", cos_sim[0][1])



----------------------------------------------------------------------------------




Lab 7: POS Tagging using HMM and Viterbi Algorithm
Question:

To develop a python code to perform POS tagging based on Hidden Markov Model and Viterbi algorithm.


import math
from collections import defaultdict

# Step 1: Training data (word, POS tag pairs)
training_data = [
    [("The", "Determiner"), ("small", "Adjective"), ("dog", "Noun, singular")],
    [("quickly", "Adverb"), ("jumped", "Verb, past tense"), ("over", "Preposition"),
     ("the", "Determiner"), ("fence", "Noun, singular")],
    [("I", "Pronoun"), ("eat", "Verb, base form"), ("pizza", "Noun, singular")],
    [("John", "Proper Noun, singular"), ("runs", "Verb, 3rd person singular present"), ("fast", "Adverb")],
    [("He", "Pronoun"), ("is", "Verb, 3rd person singular present"), ("playing", "Verb, gerund or present participle"),
     ("football", "Noun, singular")],
    [("The", "Determiner"), ("dog", "Noun, singular"), ("barks", "Verb, 3rd person singular present")]
]

# Step 2: Count occurrences of transitions, emissions, and initial states
transition_probs = defaultdict(lambda: defaultdict(int))
emission_probs = defaultdict(lambda: defaultdict(int))
initial_probs = defaultdict(int)

# Calculate the counts for transitions, emissions, and initial occurrences
for sentence in training_data:
    prev_tag = None
    for word, tag in sentence:
        emission_probs[tag][word] += 1
        if prev_tag:
            transition_probs[prev_tag][tag] += 1
        prev_tag = tag
    initial_probs[sentence[0][1]] += 1

# Convert counts to probabilities
total_sentences = len(training_data)

for tag in initial_probs:
    initial_probs[tag] /= total_sentences

for prev_tag in transition_probs:
    total_transitions = sum(transition_probs[prev_tag].values())
    for tag in transition_probs[prev_tag]:
        transition_probs[prev_tag][tag] /= total_transitions

for tag in emission_probs:
    total_emissions = sum(emission_probs[tag].values())
    for word in emission_probs[tag]:
        emission_probs[tag][word] /= total_emissions

# Step 3: Define the Viterbi algorithm
def viterbi_algorithm(words, tags, transition_probs, emission_probs, initial_probs):
    V = [{}]
    path = {}

    # Initialize base cases (start with the initial probabilities)
    for tag in tags:
        if words[0] in emission_probs[tag]:
            V[0][tag] = math.log(initial_probs.get(tag, 1e-6)) + math.log(emission_probs[tag].get(words[0], 1e-6))
        else:
            V[0][tag] = math.log(initial_probs.get(tag, 1e-6)) + math.log(1e-6)  # Smoothing
        path[tag] = [tag]

    # Run Viterbi for subsequent words
    for t in range(1, len(words)):
        V.append({})
        new_path = {}

        for tag in tags:
            max_prob, prev_tag = max(
                (V[t-1][prev_tag] + math.log(transition_probs.get(prev_tag, {}).get(tag, 1e-6)) +
                 math.log(emission_probs[tag].get(words[t], 1e-6)), prev_tag)
                for prev_tag in tags
            )
            V[t][tag] = max_prob
            new_path[tag] = path.get(prev_tag, []) + [tag]

        path = new_path

    # Find the final most probable path
    max_final_prob, final_tag = max((V[len(words)-1][tag], tag) for tag in tags)
    return path[final_tag]

# Step 4: Test sentence
test_sentence = ["The", "small", "dog", "quickly", "jumped", "over", "the", "fence"]

# Tags from training data
tags = [
    "Determiner", "Adjective", "Noun, singular", "Adverb", "Verb, past tense",
    "Preposition", "Pronoun", "Verb, base form", "Proper Noun, singular",
    "Verb, 3rd person singular present", "Verb, gerund or present participle"
]

# Run Viterbi to predict the POS tags for the test sentence
predicted_tags = viterbi_algorithm(test_sentence, tags, transition_probs, emission_probs, initial_probs)

# Step 5: Print result
print(f"Sentence: {' '.join(test_sentence)}")
print(f"Predicted POS tags: {predicted_tags}")




--------------------------------------------------------------------------------




Experiment 8:To develop a python code to analyse PCFG.
Question:
Exercise:
To develop a python code to analyse PCFG.





import nltk
from nltk import PCFG

# Define the probabilistic grammar
grammar = nltk.PCFG.fromstring("""
S -> NP VP [1.0]
VP -> V NP [0.5] | V NP PP [0.5]
PP -> P NP [1.0]
V -> "saw" [1.0]
NP -> "John" [0.5] | "Mary" [0.5]
P -> "with" [1.0]
""")

# Create the parser
parser = nltk.ViterbiParser(grammar)

# Sentence to be parsed
sentence = ['John', 'saw', 'Mary']

# Parse the sentence and print the parse tree
for tree in parser.parse(sentence):
    print(tree)
    tree.pretty_print()





--------------------------------------------------------------------------------



Lab 9: Chunking
Question:

To develop a Python code to extract meaningful phrases (chunks) from unstructured text data.

import spacy

# Load the pre-trained spaCy model
nlp = spacy.load("en_core_web_sm")

# Sample text for Named Entity Recognition
text = """
Apple Inc. is planning to open a new store in New York City.
Tim Cook, the CEO of Apple, will lead the event scheduled for October 12, 2025.
"""

# Process the text through spaCy pipeline
doc = nlp(text)

# Extract named entities
for entity in doc.ents:
    print(f"Entity: {entity.text}, Label: {entity.label_}")




--------------------------------------------------------------------------------




Lab 10: Named Entity Recognition
Question:

To develop a python code to perform Named Entity Recognition using spacy’s pre-trained model.


import spacy

# Load the pre-trained spaCy model
nlp = spacy.load("en_core_web_sm")

# Example unstructured text
text = """
The quick brown fox jumps over the lazy dog near the big tree.
Apple Inc. is one of the leading tech companies in Silicon Valley.
Elon Musk, the CEO of Tesla, is a well-known entrepreneur.
"""

# Process the text using spaCy
doc = nlp(text)

# Extract and print noun phrases (chunks)
print("Extracted Noun Phrases (Chunks):")
for chunk in doc.noun_chunks:
    print(f"- {chunk.text}")
